project_name: rft
run_name: rft

# training setup
use_wandb: True # set to false if you don't want to log to wandb
train: True
batch_size: 200
epochs: 200
gpu_ids: [0]
num_workers: 12
lr: 1e-4
optimizer: adamw
clipping: False
max_norm: 1.
scheduler: "cosine"
warmup: True 
warmup_epochs: 4
cyclic_period: 10
plateau_patience: 3
plateau_factor: 0.5
seed: 0

# model params
model: rft
model_type: rft
efficientnet_model: efficientnet-b0
clip_model_type: ViT-B/32
encoding_size: 128
lang_encoding_size: 512
mha_num_attention_heads: 4
mha_num_attention_layers: 4
mha_ff_dim_factor: 4
vocab_size: 128
num_tokens: 8
dropout: 0.5

# normalization for the action space
normalize: True

# context
context_type: temporal
context_size: 5 # 5
alpha: 1e-4

# distance bounds for distance and action and distance predictions 
distance:
  min_dist_cat: 0
  max_dist_cat: 40
action:
  min_dist_cat: 5
  max_dist_cat: 40

# action output params
len_traj_pred: 20
learn_angle: False

# dataset specific parameters
image_size: [96, 96] # width, height
datasets:
  sacson:
    data_folder: /hdd/sacson_language_1
    train: /home/noam/LLLwL/lcbc/data/data_splits/sacson/train/ # path to train folder with traj_names.txt
    test: /home/noam/LLLwL/lcbc/data/data_splits/sacson/test/ # path to test folder with traj_names.txt
    end_slack: 3 # because many trajectories end in collisions
    goals_per_obs: 1 # how many goals are sampled per observation
    negative_mining: True # negative mining from the ViNG paper (Shah et al.)

# logging stuff
## =0 turns off
print_log_freq: 100 # in iterations
image_log_freq: 1000 #0 # in iterations
num_images_log: 8 #0 
pairwise_test_freq: 0 # in epochs
eval_fraction: 0.25
wandb_log_freq: 10 # in iterations
eval_freq: 1 # in epochs